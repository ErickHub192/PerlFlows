# ğŸ¯ TOKEN TRACKING - GUÃA DE INTEGRACIÃ“N

## âœ… Sistema Listo Para Usar

El sistema de token tracking estÃ¡ **completamente configurado** y **listo para usar**. Solo necesitas agregar **2 lÃ­neas de cÃ³digo** en tus servicios existentes.

## ğŸš€ IntegraciÃ³n en ChatService

### Antes (cÃ³digo actual):
```python
# app/services/chat_service_clean.py
async def handle_message(self, chat_id, user_message, user_id, context=None, db_session=None):
    # Tu cÃ³digo existente...
    
    # Llamada LLM sin tracking
    result = await self.llm.run(
        system_prompt=prompt,
        short_term=short_term,
        long_term=long_term,
        user_prompt=user_message
    )
    
    return ChatResponseModel(reply=result["final_output"])
```

### DespuÃ©s (con auto-tracking):
```python
# app/services/chat_service_clean.py
from app.ai.llm_clients.llm_service import token_tracking_context  # â† Agregar import

async def handle_message(self, chat_id, user_message, user_id, context=None, db_session=None):
    # â† Agregar context manager
    async with token_tracking_context(
        user_id=user_id,
        operation_type="chat", 
        workflow_id=str(chat_id)
    ):
        # Todo tu cÃ³digo existente aquÃ­ - SIN CAMBIOS
        result = await self.llm.run(
            system_prompt=prompt,
            short_term=short_term,
            long_term=long_term,
            user_prompt=user_message
        )
        
        # ğŸ“Š â† Tokens automÃ¡ticamente tracked!
        
        return ChatResponseModel(reply=result["final_output"])
```

## ğŸš€ IntegraciÃ³n en WorkflowRunner

### Para WorkflowRunnerService:
```python
# app/services/workflow_runner_service.py
from app.ai.llm_clients.llm_service import token_tracking_context

async def run_workflow(self, flow_id, steps, user_id, inputs, simulate=False):
    # â† Agregar context para todo el workflow
    async with token_tracking_context(
        user_id=user_id,
        operation_type="workflow",
        workflow_id=str(flow_id),
        execution_id=str(execution_id)
    ):
        # Todo tu cÃ³digo de workflow existente
        # Cualquier call LLM interno serÃ¡ tracked automÃ¡ticamente
        return await self._execute_workflow_steps(steps)
```

## ğŸš€ IntegraciÃ³n en AI Agents

### Para AIAgentService:
```python
# app/services/ai_agent_service.py  
from app.ai.llm_clients.llm_service import token_tracking_context

async def execute_agent(self, agent_id, prompt, user_id):
    async with token_tracking_context(
        user_id=user_id,
        operation_type="ai_agent",
        workflow_id=str(agent_id)
    ):
        # Tu lÃ³gica de AI agent
        # Todos los LLM calls serÃ¡n tracked
        return await self._run_agent_logic(prompt)
```

## ğŸ“Š Verificar LÃ­mites (Opcional)

### Si quieres verificar lÃ­mites antes de ejecutar:
```python
from app.core.token_system import get_token_manager

async def handle_message(self, chat_id, user_message, user_id):
    # Verificar lÃ­mites primero (opcional)
    token_manager = get_token_manager()
    status = await token_manager.can_use_tokens(user_id, estimated_tokens=2000)
    
    if not status.can_use:
        return ChatResponseModel(
            reply=f"âŒ LÃ­mite de tokens alcanzado. Remaining: {status.remaining_tokens}",
            finalize=True
        )
    
    # Proceder con tracking automÃ¡tico
    async with token_tracking_context(user_id=user_id, operation_type="chat"):
        # Tu cÃ³digo LLM...
```

## ğŸ¯ Lo Que Se Registra AutomÃ¡ticamente

Cada vez que se ejecuta cÃ³digo dentro del `token_tracking_context()`:

âœ… **Input tokens** (prompt enviado)  
âœ… **Output tokens** (respuesta recibida)  
âœ… **Modelo usado** (gpt-4.1, gpt-4o, etc.)  
âœ… **Costo calculado** (based on current pricing)  
âœ… **User ID, Workflow ID, Execution ID**  
âœ… **Timestamp y metadata**  

## ğŸ“ˆ Analytics Disponibles

El sistema automÃ¡ticamente genera:

- **Uso por usuario/mes** 
- **Tokens por workflow**
- **Costos por operaciÃ³n**
- **Alertas de lÃ­mites** (80%, 90%, 100%)
- **Top workflows que mÃ¡s consumen**
- **Tendencias de uso diario**

## ğŸš¨ Alertas AutomÃ¡ticas

El sistema envÃ­a alertas automÃ¡ticamente cuando:

- Usuario usa 80% de sus tokens â†’ Email/notificaciÃ³n  
- Usuario usa 90% de sus tokens â†’ Warning  
- Usuario alcanza 100% â†’ Bloqueo suave  

## ğŸ”§ ConfiguraciÃ³n

### Development (automÃ¡tico):
- Batch size: 5 tokens per batch
- Alertas: Solo in-app notifications

### Production (automÃ¡tico):  
- Batch size: 20 tokens per batch
- Alertas: Email + webhooks + in-app

## âœ… Ready to Use!

1. **Sistema inicializado** âœ… en startup
2. **Tablas creadas** âœ… con migraciÃ³n  
3. **Interceptor activo** âœ… en LLMService
4. **Solo agregar context manager** âœ… en tus servicios

**2 lÃ­neas de cÃ³digo = Token tracking completo** ğŸ‰