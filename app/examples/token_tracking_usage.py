# app/examples/token_tracking_usage.py

"""
üéØ EJEMPLOS DE USO DEL SISTEMA DE TOKEN TRACKING AUTOM√ÅTICO

El sistema intercepta AUTOM√ÅTICAMENTE todos los tokens del LLM sin modificar c√≥digo existente.
Solo necesitas establecer el contexto antes de usar el LLM.
"""

import asyncio
from uuid import uuid4
from typing import Dict, Any

# Imports del sistema de tokens
from app.core.token_system import initialize_token_system, get_token_manager
from app.ai.llm_clients.llm_service import (
    get_llm_service, 
    token_tracking_context,
    set_token_context,
    clear_token_context
)

async def example_1_context_manager():
    """
    ‚úÖ M√âTODO RECOMENDADO: Context manager autom√°tico
    """
    print("üéØ Ejemplo 1: Context Manager (RECOMENDADO)")
    
    user_id = 123
    workflow_id = str(uuid4())
    
    # Todo lo que est√© dentro del context ser√° tracked autom√°ticamente
    async with token_tracking_context(
        user_id=user_id,
        operation_type="workflow",
        workflow_id=workflow_id
    ):
        # Cualquier llamada LLM aqu√≠ ser√° interceptada autom√°ticamente
        llm_service = get_llm_service()
        
        result = await llm_service.run(
            system_prompt="You are a helpful assistant",
            short_term=[],
            long_term=[],
            user_prompt="Explain what is machine learning in 50 words",
            temperature=0.7
        )
        
        # Los tokens se registran autom√°ticamente en background
        print(f"‚úÖ LLM call completed. Result: {result.get('final_output', 'No output')[:100]}...")
        print("üìä Tokens were automatically tracked!")

async def example_2_manual_context():
    """
    ‚úÖ M√âTODO MANUAL: Set/clear manual del contexto
    """
    print("\nüéØ Ejemplo 2: Manual Context")
    
    user_id = 456
    chat_id = str(uuid4())
    
    try:
        # Establecer contexto manualmente
        set_token_context(
            user_id=user_id,
            operation_type="chat",
            workflow_id=chat_id
        )
        
        # Todas las llamadas LLM ser√°n tracked
        llm_service = get_llm_service()
        
        result = await llm_service.run(
            system_prompt="You are a coding assistant",
            short_term=[],
            long_term=[],
            user_prompt="Write a Python function to calculate fibonacci",
            temperature=0.0
        )
        
        print(f"‚úÖ LLM call completed. Auto-tracked!")
        
    finally:
        # IMPORTANTE: Limpiar contexto
        clear_token_context()

async def example_3_chat_service_integration():
    """
    üîß INTEGRACI√ìN CON CHAT SERVICE
    Muestra c√≥mo integrar en servicios existentes
    """
    print("\nüéØ Ejemplo 3: Integraci√≥n con ChatService")
    
    user_id = 789
    chat_id = uuid4()
    
    # Simular ChatService.handle_message()
    async def mock_chat_service_handle_message(chat_id, user_message, user_id):
        # En tu ChatService real, agregar√≠as estas l√≠neas:
        async with token_tracking_context(
            user_id=user_id,
            operation_type="chat",
            workflow_id=str(chat_id)
        ):
            # Todo el procesamiento LLM existente aqu√≠
            llm_service = get_llm_service()
            
            result = await llm_service.run(
                system_prompt="You are a helpful chatbot",
                short_term=[],
                long_term=[],
                user_prompt=user_message,
                temperature=0.5
            )
            
            return {"reply": result.get("final_output", "No response")}
    
    # Usar el mock
    response = await mock_chat_service_handle_message(
        chat_id=chat_id,
        user_message="What is the weather like?",
        user_id=user_id
    )
    
    print(f"‚úÖ Chat response: {response['reply'][:100]}...")
    print("üìä Tokens automatically tracked in ChatService!")

async def example_4_workflow_service_integration():
    """
    üîß INTEGRACI√ìN CON WORKFLOW SERVICE
    """
    print("\nüéØ Ejemplo 4: Integraci√≥n con WorkflowService")
    
    user_id = 101
    workflow_id = str(uuid4())
    execution_id = str(uuid4())
    
    # Simular WorkflowRunner.execute_step()
    async def mock_workflow_execute_step(step_data: Dict[str, Any]):
        # En tu WorkflowRunner real, agregar√≠as estas l√≠neas:
        async with token_tracking_context(
            user_id=user_id,
            operation_type="workflow",
            workflow_id=workflow_id,
            execution_id=execution_id
        ):
            # Procesamiento del step que usa LLM
            if step_data.get("requires_llm"):
                llm_service = get_llm_service()
                
                result = await llm_service.run(
                    system_prompt="Process this workflow step",
                    short_term=[],
                    long_term=[],
                    user_prompt=f"Execute step: {step_data['action']}",
                    temperature=0.0
                )
                
                return {"status": "completed", "result": result.get("final_output")}
            
            return {"status": "completed", "result": "Step executed without LLM"}
    
    # Simular ejecuci√≥n de m√∫ltiples steps
    steps = [
        {"action": "send_email", "requires_llm": True},
        {"action": "update_database", "requires_llm": False},
        {"action": "generate_report", "requires_llm": True}
    ]
    
    for i, step in enumerate(steps):
        result = await mock_workflow_execute_step(step)
        print(f"‚úÖ Step {i+1} completed: {step['action']}")
        if step.get("requires_llm"):
            print("üìä LLM tokens automatically tracked!")

async def example_5_check_usage_and_limits():
    """
    üìä VERIFICAR USO Y L√çMITES
    """
    print("\nüéØ Ejemplo 5: Verificar Uso y L√≠mites")
    
    token_manager = get_token_manager()
    user_id = 123
    
    # Verificar si el usuario puede usar tokens
    status = await token_manager.can_use_tokens(user_id, estimated_tokens=1000)
    
    print(f"Can use tokens: {status.can_use}")
    print(f"Remaining tokens: {status.remaining_tokens}")
    print(f"Usage percentage: {status.usage_percentage:.1f}%")
    print(f"Plan type: {status.plan_type}")
    
    if not status.can_use:
        print(f"‚ùå Limit exceeded: {status.reason}")
    else:
        print("‚úÖ User can proceed with LLM operation")

async def main():
    """
    Ejecutar todos los ejemplos
    """
    print("üöÄ INICIANDO EJEMPLOS DE TOKEN TRACKING\n")
    
    # NOTA: En tu app real, esto se hace en el startup
    # initialize_token_system(db_session)
    
    try:
        await example_1_context_manager()
        await example_2_manual_context()
        await example_3_chat_service_integration()
        await example_4_workflow_service_integration()
        await example_5_check_usage_and_limits()
        
        print("\nüéâ TODOS LOS EJEMPLOS COMPLETADOS")
        print("\nüìä RESUMEN:")
        print("- Los tokens se capturan AUTOM√ÅTICAMENTE en todas las llamadas LLM")
        print("- Solo necesitas establecer el contexto con user_id")
        print("- El sistema maneja l√≠mites, alertas y analytics autom√°ticamente")
        print("- Zero configuraci√≥n adicional en tu c√≥digo existente")
        
    except Exception as e:
        print(f"‚ùå Error en ejemplos: {e}")

if __name__ == "__main__":
    asyncio.run(main())